발표 1.

1장

matplotlib은 그래프를 그려주는 라이브러리이다 p 27
시각적으로 실험 결과 확인가능

리스트는 [ ] 로 표현한다 p 30
리스트는 [0 : 2]로 자료 꺼낼 수 있다

딕셔너리는 { } 로 표현한다 p 31
me = {'height' : 180} (초기화)
me['weight'] = 70 (새 원소 추가)

numpy로 배열이나 행렬을 계산한다 p 36

넘파이 배열은 np.array()를 이용한다 p 37
x = np.array([1.0, 2.0, 3.0])

넘파이 배열끼리의 연산은 원소 수가 같으면 가능하다
브로드캐스트는 넘파이 배열의 원소별로 한번씩 스칼라 값과 연산을 수행하는 것이다

넘파이는 다차원배열도 작성할 수 있다 p 38
A = np.array([[1, 2], [3, 4]])
>>> [[1 2]
       [3, 4]]

행렬의 형상은 shape으로 알수 있고 행렬에 담긴 원소의 자료형은 dtype으로 알 수 있다 p 38
A.shape
A.dtype
>>>(2, 2)
>>>dtype('int64')

다차원 행렬의 형상이 같으면 서로 연산을 할 수 있다 p 38
*연산은 행렬의 내적이 아닌 그냥 대응하는 원소별로 곱한다
B = np.array([[3, 0], [0, 6]])
A * B
>>>[[3, 0]
      [0, 24]]

1차원 배열은 벡터, 2차원 배열은 행렬, 벡터와 행렬을 일반화한 것을 텐서라고 한다 p 39

배열의 형상이 달라도 브로드캐스트로 연산을 수행할 수 있다 p39

행렬에서 첫 인덱스는 행, 두번째 인덱스는 열을 뜻한다 p 40
X[0]
X[0][1]

행렬의 명을 호출하면 행 순서로 반환한다
for row in X:
    print(row)
>>>행1
      행2

flatten함수를 써서 다차원 배열을 1차원 배열로 만들 수 있다 p 41
X = X.flatten()

배열에서 특정 인덱스들만 뽑을 수 있다
X[np.array([0, 2, 4])] (0, 2, 4번 인덱스 출력한다)

배열에서 특정 조건을 만족하는 원소만 얻을 수 있다 p 41
X = np.array([51, 55, 14, 19, 0, 4])
X > 15
>>>array([True, True, False, True, False, False], dtype = bool)
X[X>15]
>>>array([51, 55, 19])

그래프를 그리려면 matplotlib의 pyplot 모듈을 사용한다 p 42
import matplotlib.pyplot as plt
x = np.arange(0, 6, 0.1) (0에서 6까지 0.1간격으로 생성)
y = np.sin(x)
plt.plot(x, y)
plt.show()
>>> 그래프 나온다

pyplot의 다른 기능으로 점선으로 그리고 각 축의 이름을 표시하고 그래프 제목 붙이고 x축, y축 이름을 부여할 수 있다 p 43
plt.plot(x, y1, label="sin")
plt.plot(x, y2, linestyle="--", label="cos") (점선으로 표현하고 cos함수 점선으로 그리기)
plt.xlabel("x") (x 축 이름)
plt.ylabel("y") (y 축 이름)
plt.title('sin & cos') (제목)

numpy 함수들

행렬 생성
np.array() - 입력받은 파이썬 리스트를 ndarray 형태로 바꿔준다
np.arange() - 파이썬 range 함수와 같지만 ndarray 객체를 생성한다 (하나의 인자만 입력하면 0~입력한 인자의 크기를 가진다)
np.zeros() - 입력받은 행렬의 크기만큼 0으로 채워서 행렬을 생성한다 (np.zeros((2,3)))
np.ones() - 입력받은 행렬의 크기만큼 1으로 채워서 행렬을 생성한다 (np.ones((2,3)))
np.full() - 입력받은 행렬의 크기만큼 행렬을 만들고 뒤에 입력한 숫자로 채운다 (np.full((2,3), 55))
np.eye() - 입력받은 행렬의 크기만큼의 행렬을 단위 행렬로 만든다 (np.eye(2))
np.reshape() - 행렬의 차원을 바꾸는데 사용, -1을 넣어 1차원 행렬을 만들 수 있다
np.shape() - 몇 차원 행렬인지 반환한다
np.transpose() - 행렬의 전치행렬을 구하는 것이지만 다른 기능으로 3차원 이상의 행렬에서 우선순위를 정해줄 수 있다


Array boolean 인덱싱(마스킹)
- names가 beomwoo인 행의 data만 볼 수 있다
data = np.random.randn(8,4)
names = np.array(['Beomwoo','Beomwoo','Kim','Joan','Lee','Beomwoo','Park','Beomwoo'])
names_Beomwoo_mask = (names == 'Beomwoo') (마스크를 만들면 Beomwoo인 것에 대한 boolean 값으로 행렬을 표현한다)
data[names_Beomwoo_mask, :]
- 마스크 없이 바로 Beomwoo만 꺼낼 수 있다
data[names == 'Beomwoo', :]

하나의 array에 적용되는 함수
np.random.randn() - 기대값이 0이고 표준편차가 1인 가우시안 정규분포를 따르는 난수를 발생시킨다 (data = np.random.randn(8, 4))
np.random.randint() - 랜덤 정수를 생성하며 범위와 반환 개수를 정해줄 수 있다.
np.abs() - 각 요소의 절대값을 계산한다
np.sqrt() - 각 요소의 제곱근을 계산한다
np.square() - 각 요소의 제곱 계산한다
np.exp() - 각 요소를 무리수 e의 지수로 삼은 값을 계산한다
np.log(), np.log10(), np.log2() - 각 요소를 자연로그, 사용로그, 밑이 2인 로그를 씌운 값을 계산한다
np.sign() - 각 요소의 부호를 계산한다(+인 경우 1, -인 경우 -1, 0인 경우 0)
np.ceil() - 각 요소의 소수 첫 번째 자리에서 올림한 값을 계산한다
np.floor() - 각 요소의 소수 첫 번째 자리에서 내림한 값을 계산한다
np.isnan() - 각 요소가 NaN인 경우 True를, 아닌 경우 False를 반환한다
np.isinf() - 각 요소가 무한대인 경우 True를, 아닌 경우 False를 반환한다
np.cos(), np.cosh(), np.sin(), np.sinh(), np.tan(), np.tanh() - 각 요소에 대해 삼각함수 값을 계산한다

두 개의 array에 적용되는 함수
np.multiply( , ), np.add( , ), np.subtract( , ), np.divide( , ) - 두 개의 array에 대해 동일한 위치의 요소끼리 연산 값을 계산한다
np.maximum( , ), np.minimum( , ) - 두 개의 array에 대해 동일한 위치의 요소끼리 비교하여 최대값 또는 최소값을 계산한다

통계 함수
np.sum() - 전체 요소의 합을 계산한다
np.sum( , axis = 1) - 열 간의 합을 계산한다
np.sum( , axis = 0) - 행 간의 합을 계산한다
np.mean() - 전체 요소의 평균을 계산한다
np.mean( , axis = 0) -행 간의 평균을 계산한다
np.std(), np.var(), np.min(), np.max() - 전체 요소의 표준편차, 분산, 최소값, 최대값 계산한다
np.argmin(), np.argmax() - 전체 요소의 최소값, 최대값이 위치한 인덱스를 반환한다
np.cumsum(), np.cumprod() - 맨 처음 요소부터 각 요소까지의 누적합 또는 누적곱을 계산한다

기타 함수
np.sort() - 전체 요소에 대해서 오름차순으로 정렬한다
np.sort()[::-1] - 전체 요소에 대해서 내림차순으로 정렬한다
np.sort( , axis = 0) - 행 방향으로 오름차순으로 정렬한다

matplotlib 함수

2장

퍼셉트론 - 다수의 신호를 입력으로 받아 하나의 신호를 출력하는 것 p 47
            - 흐른다/안흐른다 (1 or 2) 로 표현

입력값들과(x) 가중치값들을(w) 곱한 값이 출력신호(y)가 되는데 이 값이 임계값(세타θ) 보다 크면 1 작으면 0 출력한다 p 48
입력값 마다 고유 가중치가 있다
강중치는 각 신호가 결과에 주는 영향력을 조절하는 요소로 작용한다
가중치가 클수록 중요한 신호이다

파이썬으로 AND 퍼셉트론 구현 p 51

numpy로 AND 퍼셉트론 구현 p 52
b는 편향으로 세타에 -를 곱한후 입력과 강중치의 곱에 더한다

편향은 뉴런이 얼마나 쉽게 활성화하느냐를 조정하는 매개변수이다 p 53

AND OR NAND gate는 직선으로 표현 가능하지만 XOR gate는 곡선으로 표현해야한다 p 55 ~ p 57
직선으로 표현하는 것을 선형, 곡선으로 표현하는 것을 비선형이라고 한다
XOR gate는 퍼셉트론으로 표현할 수 없다

다층 퍼셉트론으로 XOR을 표현할 수 있다 p 58

XOR gate 파이썬으로 표현 p 59

XOR gate를 뉴런을 이용한 퍼셉트론으로 표현 p 60

발표 2.

3장

신경망은 매개변수를 자동으로 적절한 값을 설정한다 p 63

신경망은 왼쪽부터 입력층, 은닉층, 출력층으로 나뉜다 p 64

활성화 함수는 입력 신호의 총합을 출력 신호로 변환하는 함수이다 p 66

가중치 신호를 조합한 결과가 a라는 노드가 되고, 활성화 함수 h()를 통과하여 y라는 노드로 변환되는 과정이다 p 67
뉴런 = 노드 

계단 함수는 활성화 함수를 임계값을 경계로 출력을 바꾸는 식이다 p 68
퍼셉트론에서는 활성화 함수로 계단 함수를 이용한다

시그모이드 함수는 신경망에서 자주 사용하는 함수로 exp(-x)는 e^-x이다 p 68

astype()함수는 numpy 배열 타입을 변경할 수 있다 p 70

return np.array(x > 0 , dtype = no.int)로 바로 형변환하여 반환할 수 있다 p 71

시그모이드 함수와 계단 함수의 차이점은 매끄러움(연속적인 것)이다 p 74

시그모이드 함수와 계단 함수의 공통점은 둘 다 입력이 작을 때 출력은 0에 가깝고 클 때 출력이 1에 가깝다는 것이다 p 75
또한, 입력이 아무리 작거나 커도 출력은 0에서 1 사이라는 것도 둘의 공통점이다
신경망은 모두 비선형이다

ReLU() 함수는 입력이 0보다 크면 입력을 그대로 출력하고 0이하이면 0을 출력하는 함수이다 p 76

배열의 차원 수는 np.ndim()으로 알 수 있다 p 78
1차원 배열의 shape() 함수는 (열 개수, )로 표현된다
shape() 함수에서 첫 번째 인덱스는 0번째 차원의 개수를 말하고 두 번째 인덱스는 1번째 차원의 개수를 말한다

두 행렬의 내적은 np.dot() 함수로 계산한다 p 80

분류는 데이터가 어느 클래스에 속하느냐는 문제이다  p 90
회귀는 입력 데이터에서 (연속적인) 수치를 예측하는 문제이다
분류는 소프트맥스 함수, 회귀는 항등 함수를 사용한다

항등 함수는 입력을 그래도 출력한다 p 91
소프트맥스 함수에서 n은 출력층의 뉴런 수, yk는 그중 k번째 출력임을 뜻하고 분자는 입력 신호 ak의 지수 함수, 분모는 모든 입력 신호의 지수 함수의 합으로 구성된다

소프트맥스 함수는 오버플로 문제가 있기 때문에 분자와 분모에 C를 곱하는 것으로 보완한다 p 91

nan은 not a number의 약자이다 p 94
소프트맥스 함수의 출력은 0에서 1.0 사이의 실수이고 출력의 총합은 1 이다

소프트맥스 함수의 출력을 활률로 해석할 수 있다 p 95
소프트맥스 함수를 적용해도 각 원소의 대소 관계는 변하지 않는다
신경망으로 분류할 때는 출력층의 소프트맥스 함수를 생략해도 된다
기계학습의 문제 풀이는 학습과 추론으로 이루어진다
학습 단계에서 모델을 학습한다
추론 단계에서 앞서 학습한 모델로 미지의 데이터에 대해서 추론을 수행한다

load_mnist는 "(훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블)" 형식으로 반환한다 p98
첫 번째 인수 normalize는 입력 이미지의 픽셀 값을 0.0 ~ 1.0 사이의 값으로 정규화할지를 정한다
False이면 입력 이미지의 픽셀은 원래 값 그대로 0~255 사이의 값을 유지한다
두 번째 인수 flatten은 입력 이미지를 1차원 배열로 만들지를 정한다
False이면 입력 이미지를 1 * 28 * 28 의 3차원 배열로, True면 784개의 원소로 이뤄진 1차원 배열로 저장한다
세 번째 인수 one_hot_label은 레이블을 one-hot encoding 형태로 저장할지 정한다
원-핫 인코딩은 정답을 뜻하는 원소만 1이고 나머지는 모두 0인 배열을 말한다
pickle을 사용하여 특정 객체를 파일로 저장하고 나중에 사용해서 실행 시간을 단축할 수 있다
mnist의 이미지 표시에는 PIL(python image library)모듈을 사용한다

Image.fromarray()를 이용하여 넘파이로 저장된 이미지 데이터를 PIL용 데이터 객체로 변환한다 p 99

mnist는 입력층 뉴런 784개, 출력층 뉴런 10개로 구성된다 p 100
입력층은 이미지 크기가 28 * 28 이기 때문에 뉴런이 784개이고, 출력층은 0 ~ 9 숫자를 구분하므로 뉴런이 10개이다
은닉층은 총 2개로, 첫 번째 은닉층에는 뉴런 50개, 두 번째 은닉층에는 뉴런 100개가 있다 (임의로 정한 값이다)
init_network() 함수에서 이용하는 pickle 파일은 가중치와 편향 매개변수가 딕셔너리 변수로 저장되어있다

neuralnet_mnist.py는 신경망의 정확도를 측정하는 코드이다  p101
x에 저장된 이미지 데이터를 1장씩 꺼내서 predict() 함수로 분류한다
predict() 함수는 각 레이블의 확률을 numpy 배열로 반환하는데 [0.1,0.3,...., 0.04] 처럼 나오며 각 숫자는 숫자 '0', '1', ... , '9' 일 확률이다
그 후 np.argmax() 함수를 통해 확률이 가장 큰 원소의 인덱스를 구한다(즉, 숫자를 구한다)
신경망이 예측한 답변과 정답 레이블을 비교하여 맞힌 숫자를 세고, 이를 전체 이미지 숫자로 나눠 정확도를 구한다
이 코드는 normalize가 True로 정규화 되어 있다
정규화는 데이터를 특정 범위로 변환하는 처리이다
전처리는 신경망의 입력 데이터에 특정 변환을 가하는 것이다

백색화는 전체 데이터를 균일하게 분포시킨다 p 102

신경망 각 층의 배열 형상의 추이를 확인하여 1차원 배열이 출력층에 도달하는 것을 알 수 있다 p 103
이미지를 한번에 묶어서 predict() 함수에 전달할 수 있다
배치(batch)는 하나로 묶은 입력 데이터를 말한다
배치는 계산을 여러번하지 않고 한번에 하여 빠른 실행에 도움이 된다

range(start, end, step)에서 start는 시작 인덱스이고 end는 끝 + 1 인덱스이고 step은 start에서 더해지는 간격을 의미한다 p 104
batch_size를 선언하여 step의 크기를 정하여 묶음의 크기를 정한다
axis = 1은 행단위로 연산을 진행한다는 뜻이다
y == t는 두 리스트의 각 요소가 일치하는지 boolean으로 표현한 것이고 np.sum(y == t)는 boolean에서 True의 개수를 말한다
매끄럽게 변화하는 것을 표현하고 싶으면 시그모이드 함수를 사용한다

신경망 구현에서 가중치(w)의 열의 개수는 다음 가중치의 행의 개수가 된다 p 89

발표 3.

4장

기계학습은 사람이 생각한 특징(SIFT, HOG)을 설계한 것을 바탕으로 기계가학습하는 것이다(SVM, KNN) p 110
신경망(딥러닝)은 처음부터 끝까지, 데이터에서 목표한 결과를 사람의 개입 없이 얻는다

오버피팅(overfitting)은 참고로 한 데이터셋에만 지나치게 최적화된 상태이다 p 111
지표를 만들고 그 지표를 가장 좋게 만들어주는 가중치 매개변수 값을 탐색한다

손실 함수(loss function)는 신경망 학습에서 사용하는 지표이다 p 112
손실 함수는 일반적으로 오차제곱합, 교차 엔트로피 오차를 사용한다
손실 함수는 나쁨을 나타내는 지표로 낮을 수록 좋다
오차제곱합(sum of squares for error, SSE) 에서 y는 신경망의 출력(신경망이 추정한 값), t는 정답 레이블, k는 데이터의 차원수를 나타낸다
정답 레이블(t)는 원-핫 인코딩으로 나타낸다
y는 소프트맥스 함수로 출력하여 확률로 해석한다

오차제곱합을 한 후 결과값은 손실 함수이기 때문에 작을수록 정확도가 높다는 것이다 p 113

교차 엔트로피 오차(cross entropy error,CEE)는 실질적으로 정답일 때의 추정(t가 1일 때의 y)의 자연로그를 계산한 식이다 p 114
교차 엔트로피 오차는 t가 1일 때만 값이 나오므로 정답일 때에만 출력값이 나온다
교차 엔트로피 오차는 정답일 때의 출력이 작아질수록 오차가 커진다

교차 엔트로피 오차에서 np.log() 함수가 0이 되어서 -inf가 되는 것을 방지하기 위해 아주 작은 값인 delta를 넣는다 p 115

교차 엔트로피 오차의 손실 함수의 합을 구하여 평균 손실 함수를 구할 수 있다 p 116
평균을 구하기 위해 손실 함수의 합을 N으로 나눈다
tnk는 n번째 데이터의 k번째 값을 의미한다
미니배치(mini-batch)는 데이터의 양이 많을 때 훈련 데이터로부터 일부만 골라서 학습을 하는 것이다
미니배치 학습은 훈련 데이터 중 무작위로 지정한 숫자만큼 뽑아서 학습하는 것이다

교차 엔트로피 오차를 구할 때 원-핫 인코딩이 아닌 경우 t를 곱하지 않고 y[np.arange[batch_size, t]] +1e-7을 한다 p 118
교차 엔트로피 오차를 이용할 때 아직 y 값에 이전 페이지의 미니배치를 줄 순 없다 (열의 개수가 앞장은 784, 지금은 10개이다)

리스트에서 a[0][1] 은 a[0,1]과 같은 뜻이다 (둘다 행, 열 인덱스를 뜻한다) p 119
가중치 매개변수의 손실 함수의 미분(기울기)이란 가중치 매개변수의 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하나라는 의미이다 (매개변수는 y 값이다)
미분 값이 음수면 가중치 매개변수를 양의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다
미분 값이 양수면 가중치 매개변수를 음의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다
미분 값이 0이면 가중치 매개변수를 어느 쪽으로 움직여도 손실 함수의 값은 줄어들지 않으므로 갱신을 멈춘다
정확도를 지표로 삼아서는 안 되는 이유는 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없기 때문이다

미분은 '특정 순간'의 변화량을 뜻한다 p 121
미분은 x의 '작은 변화'가 함수 f(x)를 얼마나 변화시키느냐를 의미한다
h는 시간을 뜻하고 lim 0으로 수렴한다

반올림 오차는 파이썬의 소수점 한계 때문에 발생하여 h를 10e-50로 표현하지 못한다 p 122
차분은 두 점에서의 함수 값들의 차이이다
반올림 오차를 해결하기 위해 h를 10^-4 로 한다
반올림 오차를 10^-4로 해도 x위치의 기울기가 아닌 f(x+h) 와 x사이의 기울기가 결과값이 된다

중앙 차분(중심 차분)은 (x+h)와 (x-h)일 때 함수 f의 차부능ㄹ 계산하는 방법이다 (x 전 후를 계산한다) p 123
수치 미분은 아주 작은 차분으로 미분하는 것이다
해석적은 수식을 전개해 미분하는 것으로 오차를 포함하지 않는다 (진정한 미분)

편미분은 변수가 여럿인 함수에 대해 미분을 한 것이다 p 127
편미분을 할 때에는 여러 변수 중 목표 변수 하나에 초점을 맞추고 다른 변수는 값을 고정한다
나머지 변수를 고정한 새로운 함수를 정의한 후 수치 미분 함수에 새로운 함수와 목표 변수를 넣어서 계산한다

기울기는 함수의 '가장 낮은 장소(최솟값)'을 가리킨다 p 129
기울기는 '가장 낮은 곳'에서 멀어질수록 화살표의 크기가 커진다
기울기가 가리키는 족은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다
경사법은 기울기를 이용하여 함수의 최솟값을 찾으려는 것이다 (손실 함수가 최솟값이 되는 매개변수를 찾는다)

발표 4.

극솟값은 국소적인 최솟값, 즉 한정된 범위에서의 최솟값인 점이다 p 130
안정점은 어느 방향에서 보면 극댓값이고 다른 방향에서 보면 극솟값이 되는 점이다
경사법이란 현 위치에서 기울어진 방향으로 일정 거리만큼 이동하는 것이다

경사법 수식 p 131
경사법으로 신경망 매개변수를 갱신할 수 있다
학습률은 매개변수 값을 얼마나 갱신할 것인지 정하는 것이다
gradient_descent 함수에서 f는 최적화하려는 함수, init_x는 초깃값, lr은 learning rate으로 학습률, step_num은 경사법에 따른 반복 횟수이다
경사법 이후에는 기울기가 가장 낮은 장소인 원점에 가까워져서 적절한 매개변수를 구할 수 있다

하이퍼파라미터는 사람이 직접 설정해야하는 매개변수로 학습률이 있고 가중치 매개변수는 신경망에서 자동으로 학습된다 p 133
신경망에서 기울기의 의미는 매개변수의 변경에 따른 손실함수의 변화이다

신경망의 예측값을 구한 후 손실함수를 구하고 numerical_gradient 함수로 기울기를 구한다 p 134

기울기의 결과값은 매개변수를 특정 수치만큼 늘리면 결과값만큼 증감한다는 뜻이다 p 135

f() 함수를 람다로 표현하는 것이 편하다 p 136

확률적 경사 하강법(SGD)은 데이터를 미니배치로 무작위로 선정하여 수행하는 경사 하강법이다 p 137
2층 신경망의 기울기를 계산하는 코드

2층 신경망의 기울기를 계산하는 코드에서 변수와 메서드 p 139
params는 신경망의 매개변수를 보관하는 딕셔너리 변수이다
grads는 기울기 보관하는 딕셔너리 변수이다
__init__는 생성자로 인수는 순서대로 뉴런 수, 은닉층의 뉴런 수, 출력층의 뉴런 수 이다
predict(self, x)는 예측(추론)하는 함수이다
loss(self, x, t)는 손실 함수의 값을 구한다 (인수 x는 이미지 데이터, t는 정답 레이블)
accuracy(self, x, t)는 정확도를 구하는 함수이다
numerical_gradient(self, x, t)는 가중치 매개변수의 기울기를 구한다
gradient(self, x, t)는 가중치 매개변수의 기울기를 구한다(numerical_gradient()의 성능 개선판)

오차역전파법은 기울기 계산을 고속으로 수행하는 기법이다 p 141
TwoLayerNet 과 MNIST 학습 코드
가중치 매개변수를 갱신하여 손실 함수를 줄이는게 목표이다

시험데이터로 평가하는 코드 p 143
에폭(epoch)은 하나의 단위로 1에폭은 학습에서 훈련 데이터를 모두 소진했을 대의 횟수에 해당한다( ex. 10000개를 100개의 미니배치로 학습할 경우, 확률적 경사 하강법을 10000/100회 반복하면 모든 훈련 데이터를 소진하므로 여기서 1에폭 = 100회)

accuracy()함수는 가중치 매개변수가 핵심이므로 오버피팅이 일어나지 않는다면 훈련 데이터와 시험 데이터의 정확도는 거의 같아진다 p 145

발표 4.

5장

오차역전파법은 오차를 역으로 전파하는 방법으로 가중치 매개변수의 기울기를 효율적으로 계산하는 방법이다 p 147

계산 그래프는 계산 과정을 그래프로 나타낸 것으로 노드와 에지로 표현된다 p148

순전파는 계산을 왼쪽에서 오른쪽으로 진행하는 것으로 그래프의 출발점부터 종착점으로의 전파이다 p 150
역전파는 계산을 오른쪽에서 왼쪽으로 진행하는 것이다
국소적 계산이란 자신과 직접 관계된 작은 범위를 계산하는 것으로 전체에서 어떤 일이 벌어지든 상관없이 자신과 관계된 정보만으로 결과를 출력할 수 있다는 뜻이다

계산 그래프는 역전파를 통해 미분을 휴율적으로 계산할 수 있기 때문에 사용한다 p 151

역전파를 하여 초기값으로 1을 넣어주고 반대로 계산하면 미분값이 나온다 (국소적 미분) p 152
중간단계에서 미분을 시작하여 각 변수에 대한 미분값을 구할 수 있다

역전파를 계산하는 방법은 계산 함수를 미분하고 역으로 오는 값을 그 식에 대입하여 미분값을 구하는 것이다 p 153
합성 함수란 여러 함수로 구성된 함수이다 (함수를 세분화하여 합성 함수 여부를 알 수 있다)
연쇄법칙은 "합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낸다"는 정의를 가지고 있다

연쇄법칙을 적용한 식 p 154

덧셈 노드 역전파는 상류의 미분값에 1을 곱하여 그대로 하류로 보내준다 (계수없는 변수의 미분값은 1 이다) p 156

곱셈 노드 역전파는 상류의 미분값에 입력 신호를 서로 바꾼 값을 곱하여 하류에 보내준다 (순전파 때 x였다면 역전파에서는 y, 순전파 때 y였다면 역전파에서는 x로 바꾼다) p 158

각 변수에 대한 미분값은 그 변수가 1 늘어났을 때 최종 결과가 미분값 만큼 변한다는 뜻이다 p159

계층은 신경망의 기능 단위로 Sigmoid 계층, Affine 계층 등이 있다 p 160
MulLayer 클래스는 곱셈 계층, AddLayer 클래스는 덧셈 계층을 뜻한다
forward() 함수는 순전파, backward() 함수는 역전파이다

MulLayer 클래스의 순전파와 역전파 계산 코드 p 161

사과 쇼핑을 순전파와 역전파 코드로 구현 p 162
backward() 함수 호출 순서는 forward() 함수 때와 반대이다

AddLayer 클래스의 순전파와 역전파 계산 코드 p 163

발표 5.

